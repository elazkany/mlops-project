name: Train, Evaluate and Report

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main

permissions: write-all

jobs:
  train_and_report_eval_performance:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout 
        uses: actions/checkout@v3
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: 3.10.12

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt  # Or wherever flake8 is listed

      - name: Lint with flake8 # The flake8 library was installed as a dependency in the requirements.txt file
        run: flake8 src tests --statistics

      - name: Run unit tests with pytest
        run: PYTHONPATH=src pytest -v --cov=src --cov-report=term-missing

      # Setup CML GitHub Action
      - name: Set up CML
        uses: iterative/setup-cml@v1

      - name: Set PYTHONPATH
        run: echo "PYTHONPATH=$PWD" >> $GITHUB_ENV

      - name: Reproduce DVC pipeline
        run: |
          echo "ðŸ§ª Running DVC pipeline..."
          dvc repro

      - name: Commit and push DVC-tracked outputs
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Set generic Git identity using GitHub Actions-provided variables
          git config --global user.name "${{ github.actor }}"
          git config --global user.email "${{ github.actor }}@users.noreply.github.com"

          dvc push

          git commit -m "Update metrics and plots from CI" || echo "Nothing to commit"
          git push || echo "Nothing to push"

      - name: Write CML report
        env:
          REPO_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "# ðŸ§ª Model Evaluation Report" >> model_eval_report.md

          echo "## ðŸ” Model Comparison" >> model_eval_report.md
          
          {
            echo "### ðŸ“Š Metrics Comparison"
            echo "| Model | Accuracy | F1 Score | Precision | Recall |"
            echo "|--------|----------|----------|------------|--------|"
            jq -r 'to_entries[] | [.key, (.value.accuracy // "NA"), (.value.f1_score // "NA"), (.value.precision // "NA"), (.value.recall // "NA")] | @tsv' reports/compare_metrics.json |
              while IFS=$'\t' read -r model acc f1 prec rec; do
                echo "| $model | $acc | $f1 | $prec | $rec |"
              done
          } >> model_eval_report.md

          echo "### Confusion Matrices" >> model_eval_report.md

          echo "![Confusion Matrix Comparison](./reports/compare_confusion_matrices.png)" >> model_eval_report.md

          best_model_name=$(mlflow search-runs --experiment-name "credit-card-fraud-detection" \
            --filter-tag "best_model = 'true'" \
            --output json | jq -r '.[0].tags.run_name')


          echo "## ðŸ† Best Model" >> model_eval_report.md

          echo "### Metrics" >> model_eval_report.md

          {
            echo "| Accuracy | F1 Score | Precision | Recall |"
            echo "|----------|----------|------------|--------|"
            jq -r '[.accuracy, .f1_score, .precision, .recall] | @tsv' reports/best_model_metrics.json |
              while IFS=$'\t' read -r acc f1 prec rec; do
                echo "| $acc | $f1 | $prec | $rec |"
              done
          } >> model_eval_report.md

          echo "### Confusion Matrix" >> model_eval_report.md

          echo "![Best Model Confusion Matrix](./reports/best_model_confusion_matrix.png)" >> model_eval_report.md

          # Create comment from markdown report
          cml comment create model_eval_report.md

      

